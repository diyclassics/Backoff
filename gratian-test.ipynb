{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemma Comparions for Gratian Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.tokenize.latin.word import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tools\n",
    "\n",
    "tokenizer = WordTokenizer()\n",
    "lemmatizer = BackoffLatinLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for preprocessing\n",
    "\n",
    "import html, re\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text, lower=True, remove_list=[]):\n",
    "    \n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'[\\x1a-\\x1a]', ' ', text) # ASCII control characters\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub(r'\\x00',' ',text) #Another space problem?\n",
    "    \n",
    "    \n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humanum genus duobus regitur naturali uidelicet iure et moribus ius naturae est quod in lege et euan \n",
      "\n",
      "quia uero de naturae superfluitate sermo cepit haberi queritur an post illusionem que per somnium ac\n"
     ]
    }
   ],
   "source": [
    "# Load texts into memory; show sample\n",
    "\n",
    "with open('./Gratian1.txt','r') as f:\n",
    "    text_1 = preprocess(f.read())\n",
    "with open('./Gratian2.txt','r') as f:\n",
    "    text_2 = preprocess(f.read())\n",
    "    \n",
    "print(text_1[:100],'\\n')\n",
    "print(text_2[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in text 1: 57219\n",
      "Number of tokens in text 2: 14389\n",
      "\n",
      "Number of unique tokens in text 1: 10993\n",
      "Number of unique tokens in text 2: 4860\n"
     ]
    }
   ],
   "source": [
    "# Get tokens\n",
    "\n",
    "tokens_1 = tokenizer.tokenize(text_1)\n",
    "tokens_2 = tokenizer.tokenize(text_2)\n",
    "\n",
    "print(f'Number of tokens in text 1: {len(tokens_1)}')\n",
    "print(f'Number of tokens in text 2: {len(tokens_2)}')\n",
    "print()\n",
    "print(f'Number of unique tokens in text 1: {len(set(tokens_1))}')\n",
    "print(f'Number of unique tokens in text 2: {len(set(tokens_2))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas in text 1: 5595\n",
      "Number of unique lemmas in text 2: 2967\n",
      "\n",
      "Number of unique lemmas in either group 1 or group 2: 6595\n",
      "\n",
      "Number of lemmas in both group 1 and group 2: 1967\n",
      "\n",
      "Number of lemmas in group 1 not in group 2: 3628\n",
      "Number of lemmas in group 2 not in group 1: 1000\n"
     ]
    }
   ],
   "source": [
    "# Get lemmas\n",
    "\n",
    "lemmas_pairs_1 = lemmatizer.lemmatize(tokens_1)\n",
    "lemmas_1 = [lemma for _, lemma in lemmas_pairs_1]\n",
    "lemmas_pairs_2 = lemmatizer.lemmatize(tokens_2)\n",
    "lemmas_2 = [lemma for _, lemma in lemmas_pairs_2]\n",
    "lemmas_set_1 = set(lemmas_1)\n",
    "lemmas_set_2 = set(lemmas_2)\n",
    "\n",
    "print(f'Number of unique lemmas in text 1: {len(lemmas_set_1)}')\n",
    "print(f'Number of unique lemmas in text 2: {len(lemmas_set_2)}')\n",
    "print()\n",
    "full_1_2 = lemmas_set_1.union(lemmas_set_2)\n",
    "print(f'Number of unique lemmas in either group 1 or group 2: {len(full_1_2)}')\n",
    "print()\n",
    "shared_1_2 = lemmas_set_1.intersection(lemmas_set_2)\n",
    "print(f'Number of lemmas in both group 1 and group 2: {len(shared_1_2)}')\n",
    "print()\n",
    "diff_1_2 = lemmas_set_1.difference(lemmas_set_2)\n",
    "diff_2_1 = lemmas_set_2.difference(lemmas_set_1)\n",
    "print(f'Number of lemmas in group 1 not in group 2: {len(diff_1_2)}')\n",
    "print(f'Number of lemmas in group 2 not in group 1: {len(diff_2_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After postprocessing...\n",
      "Number of unique lemmas in text 1: 5420\n",
      "Number of unique lemmas in text 2: 2882\n",
      "\n",
      "Number of unique lemmas in either group 1 or group 2: 6379\n",
      "\n",
      "Number of lemmas in both group 1 and group 2: 1923\n",
      "\n",
      "Number of lemmas in group 1 not in group 2: 3497\n",
      "Number of lemmas in group 2 not in group 1: 959\n"
     ]
    }
   ],
   "source": [
    "# Possible ways forward?\n",
    "#\n",
    "# Postprocess lemmas? i.e. remove Morpheus artifacts, etc.\n",
    "\n",
    "lemmas_1 = preprocess(\" \".join([lemma for _, lemma in lemmas_pairs_1])).split()\n",
    "lemmas_2 = preprocess(\" \".join([lemma for _, lemma in lemmas_pairs_2])).split()\n",
    "lemmas_set_1 = set(lemmas_1)\n",
    "lemmas_set_2 = set(lemmas_2)\n",
    "\n",
    "print('After postprocessing...')\n",
    "print(f'Number of unique lemmas in text 1: {len(lemmas_set_1)}')\n",
    "print(f'Number of unique lemmas in text 2: {len(lemmas_set_2)}')\n",
    "print()\n",
    "full_1_2 = lemmas_set_1.union(lemmas_set_2)\n",
    "print(f'Number of unique lemmas in either group 1 or group 2: {len(full_1_2)}')\n",
    "print()\n",
    "shared_1_2 = lemmas_set_1.intersection(lemmas_set_2)\n",
    "print(f'Number of lemmas in both group 1 and group 2: {len(shared_1_2)}')\n",
    "print()\n",
    "diff_1_2 = lemmas_set_1.difference(lemmas_set_2)\n",
    "diff_2_1 = lemmas_set_2.difference(lemmas_set_1)\n",
    "print(f'Number of lemmas in group 1 not in group 2: {len(diff_1_2)}')\n",
    "print(f'Number of lemmas in group 2 not in group 1: {len(diff_2_1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review lemmas and set up custom backoff chain\n",
    "\n",
    "with open('lemmas.txt','w') as f:\n",
    "    for lemma in sorted(list(full_1_2)):\n",
    "        f.write(f'{lemma}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
